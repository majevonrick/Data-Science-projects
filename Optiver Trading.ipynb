{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06c8d308",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c6ffd",
   "metadata": {},
   "source": [
    "We need to develop a model capable of predicting the closing price movements for hundreds of Nasdaq listed stocks using data from the order book and the closing auction of the stock. Information from the auction can be used to adjust prices, assess supply and demand dynamics, and identify trading opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d533cc",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a746e17f",
   "metadata": {},
   "source": [
    "Stock exchanges are fast-paced, high-stakes environments where every second counts. The intensity escalates as the trading day approaches its end, peaking in the critical final ten minutes. These moments, often characterised by heightened volatility and rapid price fluctuations, play a pivotal role in shaping the global economic narrative for the day.\n",
    "\n",
    "Each trading day on the Nasdaq Stock Exchange concludes with the Nasdaq Closing Cross auction. This process establishes the official closing prices for securities listed on the exchange. These closing prices serve as key indicators for investors, analysts and other market participants in evaluating the performance of individual securities and the market as a whole.\n",
    "\n",
    "Within this complex financial landscape operates Optiver, a leading global electronic market maker. Fueled by technological innovation, Optiver trades a vast array of financial instruments, such as derivatives, cash equities, ETFs, bonds, and foreign currencies, offering competitive, two-sided prices for thousands of these instruments on major exchanges worldwide.\n",
    "\n",
    "In the last ten minutes of the Nasdaq exchange trading session, market makers like Optiver merge traditional order book data with auction book data. This ability to consolidate information from both sources is critical for providing the best prices to all market participants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03934f",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba2c728",
   "metadata": {},
   "source": [
    "In this competition, you are challenged to develop a model capable of predicting the closing price movements for hundreds of Nasdaq listed stocks using data from the order book and the closing auction of the stock. Information from the auction can be used to adjust prices, assess supply and demand dynamics, and identify trading opportunities.\n",
    "\n",
    "Your model can contribute to the consolidation of signals from the auction and order book, leading to improved market efficiency and accessibility, particularly during the intense final ten minutes of trading. You'll also get firsthand experience in handling real-world data science problems, similar to those faced by traders, quantitative researchers and engineers at Optiver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7d092",
   "metadata": {},
   "source": [
    "### Files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09258b",
   "metadata": {},
   "source": [
    "[train/test].csv The auction data. The test data will be delivered by the API.\n",
    "\n",
    "* stock_id - A unique identifier for the stock. Not all stock IDs exist in every time bucket.\n",
    "* date_id - A unique identifier for the date. Date IDs are sequential & consistent across all stocks.\n",
    "* imbalance_size - The amount unmatched at the current reference price (in USD).\n",
    "* imbalance_buy_sell_flag - An indicator reflecting the direction of auction imbalance.\n",
    "  * buy-side imbalance; 1\n",
    "  * sell-side imbalance; -1\n",
    "  * no imbalance; 0\n",
    "* reference_price - The price at which paired shares are maximized, the imbalance is minimized and the distance from the bid-ask midpoint is minimized, in that order. Can also be thought of as being equal to the near price bounded between the best bid and ask price.\n",
    "* matched_size - The amount that can be matched at the current reference price (in USD).\n",
    "* far_price - The crossing price that will maximize the number of shares matched based on auction interest only. This calculation excludes continuous market orders.\n",
    "* near_price - The crossing price that will maximize the number of shares matched based auction and continuous market orders.\n",
    "* [bid/ask]_price - Price of the most competitive buy/sell level in the non-auction book.\n",
    "* [bid/ask]_size - The dollar notional amount on the most competitive buy/sell level in the non-auction book.\n",
    "* wap - The weighted average price in the non-auction book.\n",
    "\n",
    "\n",
    "    𝐵𝑖𝑑𝑃𝑟𝑖𝑐𝑒∗𝐴𝑠𝑘𝑆𝑖𝑧𝑒+𝐴𝑠𝑘𝑃𝑟𝑖𝑐𝑒∗𝐵𝑖𝑑𝑆𝑖𝑧𝑒/(𝐵𝑖𝑑𝑆𝑖𝑧𝑒+𝐴𝑠𝑘𝑆𝑖𝑧𝑒)\n",
    "* seconds_in_bucket - The number of seconds elapsed since the beginning of the day's closing auction, always starting from 0.\n",
    "* target - The 60 second future move in the wap of the stock, less the 60 second future move of the synthetic index. Only provided for the train set.\n",
    "  * The synthetic index is a custom weighted index of Nasdaq-listed stocks constructed by Optiver for this competition.\n",
    "  * The unit of the target is basis points, which is a common unit of measurement in financial markets. A 1 basis point price move is equivalent to a 0.01% price move.\n",
    "  * Where t is the time at the current observation, we can define the target:\n",
    "𝑇𝑎𝑟𝑔𝑒𝑡=(𝑆𝑡𝑜𝑐𝑘𝑊𝐴𝑃𝑡+60𝑆𝑡𝑜𝑐𝑘𝑊𝐴𝑃𝑡−𝐼𝑛𝑑𝑒𝑥𝑊𝐴𝑃𝑡+60𝐼𝑛𝑑𝑒𝑥𝑊𝐴𝑃𝑡)∗10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b72f90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: featuretools==0.27.0 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (0.27.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from featuretools==0.27.0) (1.11.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from featuretools==0.27.0) (1.24.3)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.2.0 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from featuretools==0.27.0) (1.5.3)\n",
      "Requirement already satisfied: tqdm>=4.32.0 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from featuretools==0.27.0) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from featuretools==0.27.0) (6.0)\n",
      "Requirement already satisfied: cloudpickle>=0.4.0 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from featuretools==0.27.0) (2.2.1)\n",
      "Requirement already satisfied: distributed>=2.12.0 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from featuretools==0.27.0) (2023.6.0)\n",
      "Requirement already satisfied: dask[dataframe]>=2.12.0 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from featuretools==0.27.0) (2023.6.0)\n",
      "Requirement already satisfied: psutil>=5.6.6 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from featuretools==0.27.0) (5.9.0)\n",
      "Requirement already satisfied: click>=7.0.0 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from featuretools==0.27.0) (8.0.4)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from dask[dataframe]>=2.12.0->featuretools==0.27.0) (2023.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from dask[dataframe]>=2.12.0->featuretools==0.27.0) (23.1)\n",
      "Requirement already satisfied: partd>=1.2.0 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from dask[dataframe]>=2.12.0->featuretools==0.27.0) (1.4.0)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from dask[dataframe]>=2.12.0->featuretools==0.27.0) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from dask[dataframe]>=2.12.0->featuretools==0.27.0) (6.0.0)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from distributed>=2.12.0->featuretools==0.27.0) (3.1.2)\n",
      "Requirement already satisfied: locket>=1.0.0 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from distributed>=2.12.0->featuretools==0.27.0) (1.0.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from distributed>=2.12.0->featuretools==0.27.0) (1.0.3)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from distributed>=2.12.0->featuretools==0.27.0) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from distributed>=2.12.0->featuretools==0.27.0) (1.7.0)\n",
      "Requirement already satisfied: tornado>=6.0.4 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from distributed>=2.12.0->featuretools==0.27.0) (6.3.2)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from distributed>=2.12.0->featuretools==0.27.0) (1.26.16)\n",
      "Requirement already satisfied: zict>=2.2.0 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from distributed>=2.12.0->featuretools==0.27.0) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from pandas<2.0.0,>=1.2.0->featuretools==0.27.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from pandas<2.0.0,>=1.2.0->featuretools==0.27.0) (2023.3.post1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from importlib-metadata>=4.13.0->dask[dataframe]>=2.12.0->featuretools==0.27.0) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from jinja2>=2.10.3->distributed>=2.12.0->featuretools==0.27.0) (2.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas<2.0.0,>=1.2.0->featuretools==0.27.0) (1.16.0)\n",
      "Requirement already satisfied: heapdict in /Users/rodrigogarcia/anaconda3/lib/python3.11/site-packages (from zict>=2.2.0->distributed>=2.12.0->featuretools==0.27.0) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install featuretools==0.27.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "885c57c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6e45e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069aba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/path/to/directory/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e16ddcac",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (public_timeseries_testing_util.py, line 25)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3369\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  Input \u001b[0;32mIn [24]\u001b[0;36m in \u001b[0;35m<cell line: 1>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import public_timeseries_testing_util as pt_util\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Trading/public_timeseries_testing_util.py:25\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.input_paths: Sequence[str] =\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import public_timeseries_testing_util as pt_util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14896f61",
   "metadata": {},
   "source": [
    "### Import models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c490c114",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc19b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import datetime\n",
    "\n",
    "class Delta_prices(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \"\"\"\n",
    "    Differences and ratios between prices, also combining sizes\n",
    "    \"\"\"\n",
    "    def __init__(self, auction = True): \n",
    "        self.auction = auction\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.eval(\"delta_bid_ask = (bid_price - ask_price) / (bid_price + ask_price)\")\n",
    "        X = X.eval(\"delta_wap_ref = (wap - reference_price) / (wap + reference_price)\")\n",
    "        X = X.eval(\"delta_bid_ask_size = (bid_size - ask_size) / (bid_size + ask_size + 1)\")\n",
    "        X = X.eval(\"ratio_bid_ask_matched_size = (bid_size - ask_size) / (matched_size + 1)\")\n",
    "        X = X.eval(\"imbalance_signed = imbalance_buy_sell_flag * imbalance_size\")\n",
    "        X = X.eval(\"delta_bid_ref = (bid_price - reference_price) / (bid_price + reference_price)\")\n",
    "        X = X.eval(\"delta_ask_ref = (ask_price - reference_price) / (ask_price + reference_price)\")\n",
    "        ##############--\n",
    "        X = X.eval(\"delta_ask_wap = (ask_price - wap) / (ask_price + wap)\")\n",
    "        X = X.eval(\"delta_bid_wap = (bid_price - wap) / (bid_price + wap)\")\n",
    "        X = X.eval(\"imbalance_per_delta_bidask_price = (imbalance_signed) * (bid_price - ask_price)\")\n",
    "        X = X.eval(\"delta_imbalance_matched = (imbalance_signed - matched_size)/(matched_size + imbalance_signed)\")\n",
    "        X = X.eval(\"ratio_bid_ask_size = bid_size / ask_size\")\n",
    "\n",
    "\n",
    "        if self.auction:\n",
    "            X = X.eval(\"delta_near_far = (near_price - far_price) / (near_price + far_price)\")\n",
    "            X = X.eval(\"delta_far_ref = (far_price - reference_price) / (far_price + reference_price)\")\n",
    "            X = X.eval(\"delta_near_wap = (near_price - wap) / (near_price + wap)\")\n",
    "            X = X.eval(\"delta_near_ref = (near_price - reference_price) / (near_price + reference_price)\")\n",
    "            X = X.eval(\"delta_near_far_on_matched = (near_price - far_price) / (matched_size + 1)* 10000\") #26/10\n",
    "\n",
    "        return X\n",
    "    \n",
    "\n",
    "\n",
    "class wawap_computer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes the average wap in the set of stocks at the same time instant; then I subtract it to the wap of each stock\"\"\"\n",
    "    \"\"\"The compute the average ask size and bid size of the set of stocks at time t\"\"\"\n",
    "        \n",
    "    def __init__(self, wawap = True): \n",
    "        self.wawap = wawap\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "        if self.wawap:\n",
    "\n",
    "            def compute_w_a_wap(wap, ask_size, bid_size):\n",
    "                \n",
    "                return (wap * (bid_size + ask_size)).sum() / (bid_size + ask_size).sum()\n",
    "\n",
    "\n",
    "            _ = X.groupby(['date_id', 'seconds_in_bucket'])\\\n",
    "                .apply(lambda x : compute_w_a_wap(x.wap, x.ask_size, x.bid_size))\\\n",
    "                .reset_index().rename(columns = {0 : 'w_a_wap'})\n",
    "\n",
    "            X = X.merge(_, on = ['date_id', 'seconds_in_bucket'], validate = 'm:1')\\\n",
    "                .assign(wap_less_wawap = lambda df_ : (df_.wap - df_.w_a_wap) * 10000)\n",
    "           \n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "class grouped_aggs(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes the average wap in the set of stocks at the same time instant; then I subtract it to the wap of each stock\"\"\"\n",
    "    \"\"\"The compute the average ask size and bid size of the set of stocks at time t\"\"\"\n",
    "        \n",
    "    def __init__(self, cols = ['ask_size', 'bid_size'], funcs = ['mean', 'std']): \n",
    "        self.cols = cols\n",
    "        self.funcs = funcs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "        for func in self.funcs:\n",
    "            \n",
    "            newcols = [col + f'_{func}' for col in self.cols]\n",
    "            \n",
    "            X[newcols] = X.groupby(['date_id', 'seconds_in_bucket'])[self.cols].transform(func)\n",
    "\n",
    "        X = X.fillna(0)\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "class grouped_aggs_less(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes the average wap in the set of stocks at the same time instant; then I subtract it to the wap of each stock\"\"\"\n",
    "    \"\"\"The compute the average ask size and bid size of the set of stocks at time t\"\"\"\n",
    "        \n",
    "    def __init__(self, cols = ['ask_size', 'bid_size'], funcs = ['mean', 'std']): \n",
    "        self.cols = cols\n",
    "        self.funcs = funcs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        for col in self.cols:\n",
    "            \n",
    "            for func in self.funcs:\n",
    "\n",
    "                newcol = f'{func}_{col}_less_{col}'\n",
    "\n",
    "                X[newcol] = X[col] - X.groupby(['date_id', 'seconds_in_bucket'])[col].transform(func)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "class Imbalancer_2(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"implement the above idea\"\"\"\n",
    "\n",
    "    def __init__(self, triplets_imb2 = [['ask_size', 'bid_size', 'imbalance_signed']], triplets_imb3 = [['wap', 'reference_price', 'far_price']]):\n",
    "        self.triplets_imb2 = triplets_imb2\n",
    "        self.triplets_imb3 = triplets_imb3\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        for triplet in self.triplets_imb2:\n",
    "            X[f'imb2_{triplet[0]}_{triplet[1]}_{triplet[2]}'] = X[triplet].min(axis = 1) / X[triplet].max(axis = 1)\n",
    "            \n",
    "        for triplet in self.triplets_imb3:\n",
    "            X[f'imb3_{triplet[0]}_{triplet[1]}_{triplet[2]}'] = (X[triplet].max(axis = 1) -  X[triplet].median(axis = 1)) /\\\n",
    "                                                                (X[triplet].median(axis = 1) - X[triplet].min(axis = 1))\n",
    "\n",
    "        \n",
    "        return X\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Difference_Computer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"implement the above idea\"\"\"\n",
    "\n",
    "    def __init__(self, step = 1, cols_lag = ['wap', 'reference_price'], cols_diff = ['wap', 'reference_price'], old_features = True, deltafeats = False, deltaold = True):\n",
    "        self.step = step\n",
    "        self.cols_lag = cols_lag\n",
    "        self.cols_diff = cols_diff\n",
    "        self.old_features = old_features\n",
    "        self.deltafeats = deltafeats\n",
    "        self.deltaold = deltaold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        if self.deltafeats:\n",
    "\n",
    "            for step in range(1, self.step + 1):\n",
    "                new_cols = [feat + f'_delta_{step}' for feat in self.cols_diff]\n",
    "                \n",
    "                X[new_cols] = X.groupby([ 'stock_id'])[self.cols_diff].pct_change(periods = step)*100\n",
    "  \n",
    "        if self.old_features:\n",
    "\n",
    "            for lag in range(1, self.step + 1):\n",
    "                new_cols = [feat + f'_{lag}' for feat in self.cols_lag]\n",
    "                X[new_cols] = X.groupby([ 'stock_id'])[self.cols_lag].shift(lag)\n",
    "        \n",
    "        return X\n",
    "            \n",
    "\n",
    "class Rolling_mean(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,  cols = ['wap'], cols_range = ['wap'], macd_style = True, rolling_range = True):\n",
    "        self.cols = cols\n",
    "        self.cols_range = cols_range\n",
    "        self.macd = macd_style\n",
    "        self.rolling_range = rolling_range\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        if self.macd:\n",
    "            \n",
    "            newcols = [f'{col}_rolling_macd' for col in self.cols]\n",
    "\n",
    "            X[newcols] = X.groupby([ 'stock_id'], as_index = False)[self.cols].rolling(8).mean()[self.cols] \\\n",
    "                - X.groupby(['stock_id'], as_index = False)[self.cols].rolling(16).mean()[self.cols] #12 e 22\n",
    "            \n",
    "            \n",
    "        if self.rolling_range:\n",
    "\n",
    "            newcols_min = [f'{col}_rolling_min' for col in self.cols_range]\n",
    "            \n",
    "            X[newcols_min] = X.groupby([ 'stock_id'], as_index = False)[self.cols_range].rolling(10).min()[self.cols_range]\n",
    "            \n",
    "            newcols_max = [f'{col}_rolling_max' for col in self.cols_range]\n",
    "            \n",
    "            X[newcols_max] = X.groupby([ 'stock_id'], as_index = False)[self.cols_range].rolling(10).max()[self.cols_range]\n",
    "            \n",
    "            for col in self.cols_range:\n",
    "                X[f'{col}_rolling_range'] = X[f'{col}_rolling_max'] - X[f'{col}_rolling_min']\n",
    "                del X[f'{col}_rolling_max'], X[f'{col}_rolling_min']\n",
    "                \n",
    "        return X \n",
    "    \n",
    "class Expanding_feats(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,  cols = ['wap'], funcs = ['mean', 'median', 'min'], subtract_to_feat = True):\n",
    "        \n",
    "        self.cols = cols\n",
    "        self.funcs = funcs\n",
    "        self.subtract = subtract_to_feat\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        newcols = [f'{col}_expanding_{func}' for col in self.cols for func in self.funcs]\n",
    "        \n",
    "        X[newcols] = X.groupby([ 'date_id','stock_id'])[self.cols].expanding().aggregate(self.funcs).droplevel([0,1])\n",
    "        \n",
    "        \n",
    "        if self.subtract:\n",
    "            \n",
    "            for col in self.cols:\n",
    "                for func in self.funcs:\n",
    "                    X[f'{col}_expanding_{func}_delta'] = X[col] - X[f'{col}_expanding_{func}']\n",
    "            \n",
    "        \n",
    "        X = X.fillna(0)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    \n",
    "class Ewm_feats(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,  cols_mean = ['wap'], cols_std = ['reference_price'], span = 2):\n",
    "        \n",
    "        self.cols_mean = cols_mean\n",
    "        self.cols_std = cols_std\n",
    "        self.span = span\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "        newcols = [f'{col}_ewm_mean' for col in self.cols_mean]\n",
    "            \n",
    "        X[newcols] = X.groupby([ 'stock_id'], as_index = False)[self.cols_mean].transform(lambda x : x.ewm(span = self.span).mean())\n",
    "\n",
    "        newcols = [f'{col}_ewm_std' for col in self.cols_std]\n",
    "            \n",
    "        X[newcols] = X.groupby([ 'stock_id'], as_index = False)[self.cols_std].transform(lambda x : x.ewm(span = self.span).std())\n",
    "\n",
    "        return X\n",
    "    \n",
    "class MACD_computer_v2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,  cols = ['wap']):\n",
    "        self.cols = cols\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        newcols_MACD = [f'{col}_MACD' for col in self.cols]\n",
    "        newcols_sigline = [f'{col}_sigline' for col in self.cols]\n",
    "\n",
    "        X[newcols_MACD] = X.groupby([ 'stock_id'])[self.cols].transform(lambda x : x.ewm(span = 2).mean()) \\\n",
    "            - X.groupby(['stock_id'])[self.cols].transform(lambda x : x.ewm(span = 8).mean()) #12 e 22\n",
    "\n",
    "\n",
    "        return X\n",
    "\n",
    "stock_id_map = {0: 2, 1: 10, 2: 10, 3: 2, 4: 5, 5: 12, 6: 8, 7: 4,\n",
    " 8: 5, 9: 6, 10: 3, 11: 8, 12: 1, 13: 4, 14: 11, 15: 5,\n",
    " 16: 8, 17: 4, 18: 12, 19: 1, 20: 8, 21: 3, 22: 4, 23: 4,\n",
    " 24: 3, 25: 3, 26: 6, 27: 4, 28: 6, 29: 2, 30: 2, 31: 10,\n",
    " 32: 8, 33: 12, 34: 1, 35: 8, 36: 2, 37: 2, 38: 3, 39: 2,\n",
    " 40: 5, 41: 2, 42: 8, 43: 3, 44: 3, 45: 10, 46: 10, 47: 5,\n",
    " 48: 12, 49: 6, 50: 10, 51: 12, 52: 6, 53: 8, 54: 11, 55: 2,\n",
    " 56: 12, 57: 10, 58: 11, 59: 10, 60: 12, 61: 10, 62: 10, 63: 6,\n",
    " 64: 3, 65: 2, 66: 3, 67: 10, 68: 2, 69: 10, 70: 10, 71: 3,\n",
    " 72: 4, 73: 3, 74: 10, 75: 11, 76: 3, 77: 1, 78: 10, 79: 3,\n",
    " 80: 10, 81: 1, 82: 10, 83: 4, 84: 9, 85: 8, 86: 10, 87: 4,\n",
    " 88: 10, 89: 4, 90: 8, 91: 12, 92: 8, 93: 6, 94: 12, 95: 2,\n",
    " 96: 4, 97: 1, 98: 5, 99: 3, 100: 10, 101: 8, 102: 12, 103: 12,\n",
    " 104: 2, 105: 2, 106: 8, 107: 10, 108: 7, 109: 2, 110: 3, 111: 7,\n",
    " 112: 10, 113: 12, 114: 8, 115: 10, 116: 8, 117: 3, 118: 12, 119: 10,\n",
    " 120: 3, 121: 2, 122: 5, 123: 1, 124: 8, 125: 1, 126: 4, 127: 12,\n",
    " 128: 8, 129: 8, 130: 5, 131: 1, 132: 8, 133: 10, 134: 3, 135: 12,\n",
    " 136: 8, 137: 3, 138: 10, 139: 5, 140: 4, 141: 4, 142: 4, 143: 10,\n",
    " 144: 2, 145: 8, 146: 8, 147: 6, 148: 2, 149: 2, 150: 12, 151: 8,\n",
    " 152: 3, 153: 8, 154: 12, 155: 6, 156: 8, 157: 11, 158: 1, 159: 8,\n",
    " 160: 2, 161: 10, 162: 7, 163: 2, 164: 2, 165: 5, 166: 2, 167: 3,\n",
    " 168: 8, 169: 11, 170: 3, 171: 3, 172: 10, 173: 8, 174: 8, 175: 9,\n",
    " 176: 12, 177: 10, 178: 12, 179: 10, 180: 10, 181: 3, 182: 5, 183: 4,\n",
    " 184: 12, 185: 3, 186: 2, 187: 3, 188: 7, 189: 2, 190: 10, 191: 8,\n",
    " 192: 10, 193: 10, 194: 5, 195: 2, 196: 3, 197: 12, 198: 3, 199: 6}\n",
    "\n",
    "class Sectors(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,  map = stock_id_map, cols = ['imbalance_signed', 'matched_size', 'wap_less_wawap']):\n",
    "        self.mappa = stock_id_map\n",
    "        self.cols = cols\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X['sector'] = X['stock_id'].map(self.mappa)\n",
    "\n",
    "        newcols = [f'{col}_sector' for col in self.cols]\n",
    "        X[newcols] = X.groupby(['date_id', 'sector','seconds_in_bucket'])[self.cols].transform('mean')\n",
    "        \n",
    "        return X\n",
    "    \n",
    "class grouped_aggs_std(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "        \n",
    "    def __init__(self, cols = ['ask_size', 'bid_size']): \n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        for col in self.cols:\n",
    "            \n",
    "            newcol = f'{col}_std_{col}'\n",
    "            X[newcol] = X[col] / X.groupby(['date_id', 'seconds_in_bucket'])[col].transform('std')\n",
    "\n",
    "        \n",
    "        return X\n",
    "    \n",
    "class rankino(BaseEstimator, TransformerMixin):\n",
    "        \n",
    "    def __init__(self, cols = ['ask_size', 'bid_size']): \n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "            \n",
    "        newcols = [col + f'_rank' for col in self.cols]\n",
    "        X[newcols] = X.groupby([ 'stock_id'], as_index = False)[self.cols].rolling(5).rank(method = 'min')[self.cols].fillna(0).astype('int32')\n",
    "\n",
    "        \n",
    "        return X\n",
    "    \n",
    "\n",
    "class Fillna(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "        X.replace([np.inf, -np.inf], [99999, -99999], inplace = True)\n",
    "        X = X.fillna(0)\n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dcedf5",
   "metadata": {},
   "source": [
    "### Pipeline Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cac8cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "cols_for_diff = ['ask_price', 'bid_price',  'matched_size', 'imbalance_signed', 'reference_price',\n",
    "                 'delta_bid_ask', 'tgt_1']\n",
    "\n",
    "cols_for_lag = ['ask_price','matched_size', 'imbalance_signed', 'reference_price',\n",
    "                'near_price', 'far_price',   'delta_bid_ask', \n",
    "                'wap_less_wawap']\n",
    "\n",
    "triplets_imb2 = [['reference_price', 'far_price', 'near_price'], ['ask_size', 'bid_size', 'matched_size'], ['ask_size', 'bid_size', 'imbalance_signed']]\n",
    "\n",
    "triplets_imb3 = [ ['wap', 'ask_price', 'bid_price'], ['ask_size', 'bid_size', 'matched_size'], ['ask_size', 'bid_size', 'imbalance_signed']]\n",
    "\n",
    "cols_for_MACD = [ 'matched_size', 'imbalance_signed', 'reference_price',\n",
    "                'near_price', 'delta_bid_ask', \n",
    "                 'delta_wap_ref']\n",
    "\n",
    "cols_for_grouped_aggs = ['ask_size', 'bid_size']\n",
    "\n",
    "cols_for_grouped_aggs_less = [ 'imbalance_signed', 'reference_price', \n",
    "                              'matched_size', 'ratio_bid_ask_size','delta_wap_ref', 'delta_ask_wap', 'delta_bid_ask_size']\n",
    "\n",
    "cols_for_rolling = ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'matched_size', 'imbalance_signed', 'reference_price', \n",
    "                    'near_price', 'far_price', 'wap', 'ratio_bid_ask_size', 'delta_bid_ask']\n",
    "\n",
    "cols_for_rolling_range = ['wap', 'bid_price', 'ask_price', 'delta_bid_ask']\n",
    "\n",
    "cols_for_expanding = ['bid_price', 'ask_size',  'matched_size', 'imbalance_signed', 'reference_price', \n",
    "                      'wap', 'ratio_bid_ask_size', 'delta_bid_ask', 'delta_bid_ask_size'] #'far_price','bid_size','ask_price', \n",
    "\n",
    "cols_for_rank = ['bid_price', 'ask_size',  'matched_size', 'imbalance_signed', 'reference_price', \n",
    "                      'wap', 'ratio_bid_ask_size', 'delta_bid_ask', 'delta_bid_ask_size', 'wap_less_wawap']\n",
    "\n",
    "cols_grouped_std = [\n",
    " 'matched_size', 'far_price',  'bid_size',\n",
    "  'wap', 'delta_bid_ask',\n",
    " 'delta_wap_ref', 'delta_bid_ask_size', 'ratio_bid_ask_matched_size', 'imbalance_signed',  \n",
    "  'delta_ask_ref', 'delta_ask_wap', 'delta_bid_wap',\n",
    " 'ratio_bid_ask_size']\n",
    "\n",
    "cols_ewm_mean = ['ask_size', 'matched_size', 'ratio_bid_ask_size', 'delta_bid_ask', 'delta_bid_ask_size']\n",
    "\n",
    "cols_ewm_std = ['bid_price', 'reference_price']\n",
    "\n",
    "imbalancer = Delta_prices()\n",
    "\n",
    "wawapper = wawap_computer()\n",
    "\n",
    "diff_computer = Difference_Computer(step = 3, cols_lag = cols_for_lag, cols_diff = cols_for_diff, old_features=True,  deltafeats=True, deltaold = False)\n",
    "\n",
    "imbalancer2 = Imbalancer_2(triplets_imb2 = triplets_imb2, triplets_imb3 = triplets_imb3)\n",
    "\n",
    "rolling_computer = Rolling_mean(cols = cols_for_rolling, cols_range = cols_for_rolling_range, macd_style = False, rolling_range = True)\n",
    "\n",
    "macd_computer = MACD_computer_v2(cols =cols_for_MACD )\n",
    "\n",
    "grouped_aggs = grouped_aggs(cols = cols_for_grouped_aggs, funcs = ['mean'])\n",
    "\n",
    "grouped_aggs_less = grouped_aggs_less(cols = cols_for_grouped_aggs_less, funcs = ['mean'])\n",
    "\n",
    "expanding = Expanding_feats(cols = cols_for_expanding, subtract_to_feat = True, funcs = ['median'])\n",
    "\n",
    "rankin = rankino(cols = cols_for_rank)\n",
    "\n",
    "grouped_std = grouped_aggs_std(cols = cols_grouped_std)\n",
    "\n",
    "sector = Sectors(map = stock_id_map, cols = ['ask_size', 'bid_size', 'matched_size', 'imbalance_signed', 'reference_price'])\n",
    "\n",
    "ewm = Ewm_feats(cols_mean = cols_ewm_mean, cols_std = cols_ewm_std, span = 2)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imbalancer', imbalancer),\n",
    "    ('wawapper', wawapper),\n",
    "    ('diff_comp', diff_computer),\n",
    "    ('imbalancer2', imbalancer2),\n",
    "    #('log_computer', log_return),\n",
    "    ('macd_computer', macd_computer),\n",
    "    ('sector', sector),\n",
    "    ('ewm', ewm),\n",
    "    ('expanding', expanding),\n",
    "    ('grouped_aggs', grouped_aggs),\n",
    "    ('grouped_aggs_less', grouped_aggs_less),\n",
    "    ('rolling', rolling_computer),\n",
    "    #('rank', rankin),\n",
    "    ('grouped_std', grouped_std),\n",
    "    ('fillna', Fillna())\n",
    "    \n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "910e315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['date_id', 'row_id', 'imbalance_buy_sell_flag', 'imbalance_size', 'w_a_wap', 'time_id', 'row_id', 'target','mean_delta_wap_ref_less_delta_wap_ref', 'mean_delta_ask_wap_less_delta_ask_wap','mean_delta_bid_ask_size_less_delta_bid_ask_size'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe16cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({'stock_id' : np.int16, 'date_id' : np.int16, 'seconds_in_bucket':np.int16, 'imbalance_size':'float32',\n",
    "       'imbalance_buy_sell_flag':np.int16, 'reference_price':'float32', 'matched_size':'float32',\n",
    "       'far_price':'float32', 'near_price':'float32', 'bid_price':'float32', 'bid_size':'float32', 'ask_price':'float32',\n",
    "       'ask_size':'float32', 'wap':'float32', 'target':'float32', 'time_id':np.int16})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdbc90a",
   "metadata": {},
   "source": [
    "### Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef44b6d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optiver2023'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptiver2023\u001b[39;00m\n\u001b[1;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m optiver2023\u001b[38;5;241m.\u001b[39mmake_env()\n\u001b[1;32m      3\u001b[0m iter_test \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39miter_test()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optiver2023'"
     ]
    }
   ],
   "source": [
    "import optiver2023\n",
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53de71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query(\"date_id < 478 & date_id >= 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ef3b116",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iter_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m current_date_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m478\u001b[39m\n\u001b[1;32m      4\u001b[0m last_retraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (test, revealed_targets, sample_prediction) \u001b[38;5;129;01min\u001b[39;00m \u001b[43miter_test\u001b[49m:\n\u001b[1;32m      9\u001b[0m     test \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mreset_index(drop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m     test \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mastype({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstock_id\u001b[39m\u001b[38;5;124m'\u001b[39m : np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_id\u001b[39m\u001b[38;5;124m'\u001b[39m : np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseconds_in_bucket\u001b[39m\u001b[38;5;124m'\u001b[39m:np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimbalance_size\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimbalance_buy_sell_flag\u001b[39m\u001b[38;5;124m'\u001b[39m:np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_price\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatched_size\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfar_price\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnear_price\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbid_price\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbid_size\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mask_price\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     13\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mask_size\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwap\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'iter_test' is not defined"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "last_date_id = 477\n",
    "current_date_id = 478\n",
    "last_retraining = 0\n",
    "\n",
    "for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "\n",
    "    \n",
    "    test = test.reset_index(drop = True)\n",
    "    test = test.astype({'stock_id' : np.int16, 'date_id' : np.int16, 'seconds_in_bucket':np.int16, 'imbalance_size':'float32',\n",
    "       'imbalance_buy_sell_flag':np.int16, 'reference_price':'float32', 'matched_size':'float32',\n",
    "       'far_price':'float32', 'near_price':'float32', 'bid_price':'float32', 'bid_size':'float32', 'ask_price':'float32',\n",
    "       'ask_size':'float32', 'wap':'float32'})\n",
    "    currently_scored = test.pop('currently_scored')\n",
    "\n",
    "    df = pd.concat([df, test], axis = 0, ignore_index = True)\n",
    "\n",
    "    current_second = test.loc[0, 'seconds_in_bucket']\n",
    "\n",
    "    if len(revealed_targets) > 10:\n",
    "        revealed_targets = revealed_targets.reset_index(drop = True)\n",
    "        current_date_id = revealed_targets.loc[0, 'date_id']\n",
    "        revealed_targets = revealed_targets.drop('date_id', axis = 1)\n",
    "        if counter == 0:\n",
    "            \n",
    "            tgts = revealed_targets.astype({'stock_id' : np.int16, 'revealed_date_id': np.int16, 'revealed_time_id': np.int16, \n",
    "                                            'seconds_in_bucket': np.int16, 'revealed_target' : 'float32'})\n",
    "        else:\n",
    "            \n",
    "            tgts = pd.concat([tgts, revealed_targets]).astype({'stock_id' : np.int16, 'revealed_date_id': np.int16, 'revealed_time_id': np.int16, \n",
    "                                            'seconds_in_bucket': np.int16, 'revealed_target' : 'float32'})\n",
    "            \n",
    "        df = df.merge(tgts, how = 'left', left_on = ['stock_id', 'date_id',  'seconds_in_bucket'], \n",
    "                 right_on = ['stock_id', 'revealed_date_id', 'seconds_in_bucket'])\n",
    "        \n",
    "        df.target = df.target.fillna(df['revealed_target'])\n",
    "        \n",
    "        df = df.drop(['revealed_target', 'revealed_date_id', 'revealed_time_id', ], axis = 1)\n",
    "\n",
    "        \n",
    "    if (currently_scored.loc[0] == True) & ((current_date_id - last_retraining) > 22):\n",
    "        \n",
    "        last_retraining = current_date_id.copy()\n",
    "        df = df[original_columns]\n",
    "        df_tgt_agg = df.groupby(['date_id', 'stock_id'])['target'].agg(['mean', 'std', 'median', 'skew'])\\\n",
    "        .reset_index().rename(columns = {'mean':'tgt_1_mean', 'std':'tgt_1_std', 'median':'tgt_1_median', 'skew':'tgt_1_skew'})\n",
    "        df_tgt_agg['date_id'] += 1\n",
    "        df = df.merge(df_tgt_agg, how = 'left', on = ['date_id', 'stock_id'])\n",
    "        df['tgt_1'] = df.groupby([ 'stock_id', 'seconds_in_bucket'])['target'].shift(1)\n",
    "\n",
    "        \n",
    "        transf = pipeline.transform(df)\n",
    "        transf = transf.dropna(subset = 'target')\n",
    "\n",
    "        pred_columns = [col for col in transf.columns if col not in cols_to_drop]\n",
    "         \n",
    "        transf = transf[pred_columns + ['target'] + ['date_id']]\n",
    "        \n",
    "        #lgbm retrain\n",
    "        import lightgbm as lgb\n",
    "        train_set = lgb.Dataset(data = transf.query(f'date_id >= 130')[pred_columns], label = transf.query(f'date_id >= 130')['target'])\n",
    "        val_set = lgb.Dataset(data = transf.query(f'date_id < 130')[pred_columns], label = transf.query(f'date_id < 130')['target'])\n",
    "        del transf\n",
    "\n",
    "        \n",
    "        lgb_params = {\"objective\" : \"mae\", \"n_estimators\" : 9999, \"random_state\":1021, \n",
    "                      \"num_leaves\" : 378, \"subsample\" : 0.4, \"colsample_bytree\" : 0.4, \"learning_rate\" : 0.008\n",
    "                      }\n",
    "        \n",
    "        stopping = lgb.early_stopping(100, first_metric_only=False, verbose=True)\n",
    "        lgbmregr = lgb.train(params = lgb_params, train_set = train_set, valid_sets = [val_set], callbacks = [stopping])\n",
    "        del train_set, val_set\n",
    "        df = df[original_columns]\n",
    "        \n",
    "\n",
    "        \n",
    "    if currently_scored.loc[0] == True:\n",
    "        df1 = df.query(f\"date_id >= {current_date_id - 1}\")\n",
    "        df_tgt_agg = df1.groupby(['date_id', 'stock_id'])['target'].agg(['mean', 'std', 'median', 'skew'])\\\n",
    "        .reset_index().rename(columns = {'mean':'tgt_1_mean', 'std':'tgt_1_std', 'median':'tgt_1_median', 'skew':'tgt_1_skew'})\n",
    "        df_tgt_agg['date_id'] += 1\n",
    "        df1 = df1.merge(df_tgt_agg, how = 'left', on = ['date_id', 'stock_id'])\n",
    "        df1['tgt_1'] = df1.groupby([ 'stock_id', 'seconds_in_bucket'])['target'].shift(1)\n",
    "        df1 = df1.query(f\"(date_id >= {current_date_id - 1} & seconds_in_bucket >= 350) | date_id == {current_date_id}\")\n",
    "        \n",
    "        df1 = pipeline.transform(df1)\n",
    "        X = df1.query(f\"date_id == {current_date_id} & seconds_in_bucket == {current_second}\")\n",
    "\n",
    "        pred_columns = [col for col in X.columns if col not in cols_to_drop]\n",
    "\n",
    "        X = X[pred_columns]\n",
    "        \n",
    "\n",
    "        preds = lgbmregr.predict(X)\n",
    "        sample_prediction['target'] =  preds - preds.mean()\n",
    "\n",
    "    else:\n",
    "        sample_prediction['target'] = 0\n",
    "\n",
    "    env.predict(sample_prediction)\n",
    "    \n",
    "    \n",
    "    counter += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f2740a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
